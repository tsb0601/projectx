{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "IPython.notebook.set_autosave_interval(2000)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 2 seconds\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%autosave 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrown/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:04:49 \u001b[32m__init__.py:40 [I]\u001b[0m → Notebook logger initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import transformers\n",
    "import tokenizers\n",
    "\n",
    "from llava.constants import IGNORE_INDEX, IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\n",
    "from torch.utils.data import Dataset\n",
    "from llava.train.llava_trainer import LLaVATrainer\n",
    "\n",
    "from llava import conversation as conversation_lib\n",
    "\n",
    "from llava.mm_utils import tokenizer_image_token\n",
    "# from llava.train.gcloud_rsync_callback import GCloudRsyncCallback\n",
    "from llava.train.wandb_nan_alert_callback import NanInfAlertWandbCallback\n",
    "from llava.model import LlavaLlamaForCausalLM, LlavaMptForCausalLM\n",
    "# , \\\n",
    "#     LlavaMistralForCausalLM, LlavaCohereForCausalLM, LlavaMixtralForCausalLM\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from packaging import version\n",
    "\n",
    "from ezcolorlog import root_logger as logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"facebook/opt-125m\")\n",
    "    version: Optional[str] = field(default=\"v0\")\n",
    "    freeze_backbone: bool = field(default=False)\n",
    "    tune_mm_mlp_adapter: bool = field(default=False)\n",
    "    vision_tower: Optional[str] = field(default=None)\n",
    "    mm_vision_select_layer: Optional[int] = field(default=-1)   # default to the last layer\n",
    "    pretrain_mm_mlp_adapter: Optional[str] = field(default=None)\n",
    "    mm_projector_type: Optional[str] = field(default='linear')\n",
    "    mm_use_im_start_end: bool = field(default=False)\n",
    "    mm_use_im_patch_token: bool = field(default=True)\n",
    "    mm_patch_merge_type: Optional[str] = field(default='flat')\n",
    "    mm_vision_select_feature: Optional[str] = field(default=\"patch\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(default=None,\n",
    "                           metadata={\"help\": \"Path to the training data.\"})\n",
    "    lazy_preprocess: bool = False\n",
    "    is_multimodal: bool = False\n",
    "    image_folder: Optional[str] = field(default=None)\n",
    "    image_aspect_ratio: str = 'square'\n",
    "    image_token_len: int = 576  # (336 // 14)**2\n",
    "    image_position: int = 35  # depends on v1 conv\n",
    "\n",
    " \n",
    "@dataclass\n",
    "# class TrainingArguments(transformers.TrainingArguments):\n",
    "class TrainingArguments:\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    remove_unused_columns: bool = field(default=False)\n",
    "    freeze_mm_mlp_adapter: bool = field(default=False)\n",
    "    unfreeze_mm_vision_tower: bool = field(default=False)\n",
    "    mpt_attn_impl: Optional[str] = field(default=\"triton\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\":\n",
    "            \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    double_quant: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Compress the quantization statistics through double quantization.\"}\n",
    "    )\n",
    "    quant_type: str = field(\n",
    "        default=\"nf4\",\n",
    "        metadata={\"help\": \"Quantization data type to use. Should be one of `fp4` or `nf4`.\"}\n",
    "    )\n",
    "    bits: int = field(\n",
    "        default=16,\n",
    "        metadata={\"help\": \"How many bits to use.\"}\n",
    "    )\n",
    "    lora_enable: bool = False\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_weight_path: str = \"\"\n",
    "    lora_bias: str = \"none\"\n",
    "    mm_projector_lr: Optional[float] = None\n",
    "    group_by_modality_length: bool = field(default=False)\n",
    "    mm_vision_tower_lr: Optional[float] = None\n",
    "\n",
    "    # GCSFS\n",
    "    gcp_project: Optional[str] = field(default=None)\n",
    "    \"\"\"Can also set GCP_PROJECT environment variable.\"\"\"\n",
    "    gcs_output_dir: Optional[str] = field(default=None)\n",
    "    \"\"\"gs://<bucket>/<prefix>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ModelArguments(model_name_or_path='lmsys/vicuna-7b-v1.5', version='v1', freeze_backbone=False, tune_mm_mlp_adapter=False, vision_tower='mae-vit-l-16', mm_vision_select_layer=-2, pretrain_mm_mlp_adapter=None, mm_projector_type='mlp2x_gelu', mm_use_im_start_end=False, mm_use_im_patch_token=False, mm_patch_merge_type='flat', mm_vision_select_feature='patch'),\n",
       " DataArguments(data_path='/mnt/disks/storage/data/finetune_data/5565kL.jsonl', lazy_preprocess=True, is_multimodal=False, image_folder='/mnt/disks/storage/data/finetune_data', image_aspect_ratio='pad', image_token_len=196, image_position=35),\n",
       " TrainingArguments(cache_dir=None, optim='adamw_torch', remove_unused_columns=False, freeze_mm_mlp_adapter=False, unfreeze_mm_vision_tower=False, mpt_attn_impl='triton', model_max_length=2048, double_quant=True, quant_type='nf4', bits=16, lora_enable=False, lora_r=64, lora_alpha=16, lora_dropout=0.05, lora_weight_path='', lora_bias='none', mm_projector_lr=None, group_by_modality_length=True, mm_vision_tower_lr=None, gcp_project=None, gcs_output_dir=None))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = transformers.HfArgumentParser(\n",
    "    (ModelArguments, DataArguments, TrainingArguments))\n",
    "model_args, data_args, training_args = parser.parse_args_into_dataclasses([\n",
    "    \"--model_name_or_path\", \"lmsys/vicuna-7b-v1.5\",\n",
    "    \"--version\", \"v1\",\n",
    "    \"--data_path\", \"/mnt/disks/storage/data/finetune_data/5565kL.jsonl\",\n",
    "    \"--image_folder\", \"/mnt/disks/storage/data/finetune_data\",\n",
    "        \"--vision_tower\", \"mae-vit-l-16\",\n",
    "        \"--image_token_len\", \"196\",\n",
    "        # \"--vision_tower\", \"apple/DFN2B-CLIP-ViT-L-14\",\n",
    "        # \"--image_token_len\", \"256\",\n",
    "        # \"--vision_tower\", \"timm/ViT-SO400M-14-SigLIP-384\",\n",
    "        # \"--image_token_len\", \"729\",\n",
    "        # \"--vision_tower\", \"timm/ViT-SO400M-14-SigLIP\",\n",
    "        # \"--image_token_len\", \"256\",\n",
    "    # \"--vision_tower\", \"openai/clip-vit-large-patch14-336\",\n",
    "    # \"--image_token_len\", \"576\",\n",
    "    \"--mm_projector_type\", \"mlp2x_gelu\",\n",
    "    \"--mm_vision_select_layer\", \"-2\",\n",
    "    \"--mm_use_im_start_end\", \"False\",\n",
    "    \"--mm_use_im_patch_token\", \"False\",\n",
    "    \"--image_aspect_ratio\", \"pad\",\n",
    "    \"--group_by_modality_length\", \"True\",\n",
    "    # \"--bf16\", \"False\",\n",
    "    # \"--output_dir\", \"./checkpoints/dummy\",\n",
    "    # \"--num_train_epochs\", \"1\",\n",
    "    # \"--per_device_train_batch_size\", \"16\",\n",
    "    # \"--per_device_eval_batch_size\", \"4\",\n",
    "    # \"--gradient_accumulation_steps\", \"1\",\n",
    "    # \"--evaluation_strategy\", \"no\",\n",
    "    # \"--save_strategy\", \"steps\",\n",
    "    # \"--save_steps\", \"100000\",\n",
    "    # \"--save_total_limit\", \"1\",\n",
    "    # \"--learning_rate\", \"2e-5\",\n",
    "    # \"--weight_decay\", \"0.\",\n",
    "    # \"--warmup_ratio\", \"0.03\",\n",
    "    # \"--lr_scheduler_type\", \"cosine\",\n",
    "    # \"--logging_steps\", \"1\",\n",
    "    # \"--tf32\", \"False\",\n",
    "    \"--model_max_length\", \"2048\",\n",
    "    # \"--gradient_checkpointing\", \"True\",\n",
    "    # \"--dataloader_num_workers\", \"4\",\n",
    "    \"--lazy_preprocess\", \"True\",\n",
    "])\n",
    "\n",
    "model_args, data_args, training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:05:01 \u001b[33m1205543874.py:10 [W]\u001b[0m → Vision tower, loading LlavaLlamaForCausalLM: lmsys/vicuna-7b-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llama to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# copy image_token_len and image_position to model_args\n",
    "model_args.image_token_len = data_args.image_token_len\n",
    "model_args.image_position = data_args.image_position\n",
    "\n",
    "# Assuming model_args.model_name_or_path is a string that includes the model size\n",
    "model_name = model_args.model_name_or_path\n",
    "\n",
    "bnb_model_from_pretrained_args = {}\n",
    "\n",
    "logger.warning(f\"Vision tower, loading LlavaLlamaForCausalLM: {model_args.model_name_or_path}\")\n",
    "model = LlavaLlamaForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    do_sample=True,\n",
    "    torch_dtype=(None),\n",
    "    **bnb_model_from_pretrained_args\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.generation_config.do_sample = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    model_max_length=training_args.model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:05:06 \u001b[32m1421403862.py:8 [I]\u001b[0m → tokenizer id before operation is 0\n",
      "06:05:06 \u001b[32m1421403862.py:12 [I]\u001b[0m → Model Conv Version: v1\n",
      "06:05:06 \u001b[32m1421403862.py:13 [I]\u001b[0m → Default conversation version: v1\n",
      "06:05:06 \u001b[32m1421403862.py:21 [I]\u001b[0m → Default conversation version: v1\n"
     ]
    }
   ],
   "source": [
    "def log_rank0(log):\n",
    "    logger.info(log, stacklevel=2)\n",
    "\n",
    "def print_rank0(*args):\n",
    "    log = \"\"\n",
    "    for arg in args:\n",
    "        log += str(arg)\n",
    "    log_rank0(log)\n",
    "\n",
    "print_rank0(\"tokenizer id before operation is \", tokenizer.pad_token_id)\n",
    "\n",
    "log_rank0(f\"Model Conv Version: {model_args.version}\")\n",
    "log_rank0(f\"Default conversation version: {conversation_lib.default_conversation.version}\")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "if model_args.version in conversation_lib.conv_templates:\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[model_args.version]\n",
    "else:\n",
    "    logger.warning(f\"Conversation version {model_args.version} not found. Using default `vicuna_v1`\")\n",
    "    conversation_lib.default_conversation = conversation_lib.conv_templates[\"vicuna_v1\"]\n",
    "log_rank0(f\"Default conversation version: {conversation_lib.default_conversation.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06:05:06 \u001b[32m2108420027.py:1 [I]\u001b[0m → Initializing vision modules...\n",
      "06:05:06 \u001b[32mbuilder.py:41 [I]\u001b[0m → Loading **MAE** Vision Tower: mae-vit-l-16\n",
      "06:05:11 \u001b[32m_builder.py:186 [I]\u001b[0m → Loading pretrained weights from Hugging Face hub (timm/vit_large_patch16_224.mae)\n",
      "06:05:34 \u001b[32m_hub.py:180 [I]\u001b[0m → [timm/vit_large_patch16_224.mae] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "06:05:34 \u001b[32m2108420027.py:43 [I]\u001b[0m → Vision modules initialized.\n"
     ]
    }
   ],
   "source": [
    "log_rank0(\"Initializing vision modules...\")\n",
    "model_args.unfreeze_mm_vision_tower = training_args.unfreeze_mm_vision_tower\n",
    "model.get_model().initialize_vision_modules(\n",
    "    model_args=model_args,\n",
    ")\n",
    "model.config.unfreeze_mm_vision_tower = training_args.unfreeze_mm_vision_tower\n",
    "vision_tower = model.get_vision_tower()\n",
    "# vision_tower.to(dtype=torch.bfloat16 if training_args.bf16 else torch.float16, device=training_args.device)\n",
    "\n",
    "# if not training_args.unfreeze_mm_vision_tower:\n",
    "#     vision_tower.to(dtype=torch.bfloat16, device=training_args.device)\n",
    "# else:\n",
    "#     vision_tower.to(device=training_args.device)\n",
    "data_args.image_processor = vision_tower.image_processor\n",
    "data_args.is_multimodal = True\n",
    "\n",
    "model.config.image_aspect_ratio = data_args.image_aspect_ratio\n",
    "model.config.tokenizer_padding_side = tokenizer.padding_side\n",
    "model.config.tokenizer_model_max_length = tokenizer.model_max_length\n",
    "\n",
    "model.config.tune_mm_mlp_adapter = training_args.tune_mm_mlp_adapter = model_args.tune_mm_mlp_adapter\n",
    "if model_args.tune_mm_mlp_adapter:\n",
    "    log_rank0(\"Tuning multimodal mlp adapter only...\")\n",
    "    model.requires_grad_(False)\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "model.config.freeze_mm_mlp_adapter = training_args.freeze_mm_mlp_adapter\n",
    "if training_args.freeze_mm_mlp_adapter:\n",
    "    log_rank0(\"Freezing multimodal mlp adapter...\")\n",
    "    for p in model.get_model().mm_projector.parameters():\n",
    "        p.requires_grad = False\n",
    "if training_args.unfreeze_mm_vision_tower:\n",
    "    for p in model.get_model().get_vision_tower().parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "model.config.mm_use_im_start_end = data_args.mm_use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_projector_lr = training_args.mm_projector_lr\n",
    "model.config.mm_vision_tower_lr = training_args.mm_vision_tower_lr\n",
    "training_args.use_im_start_end = model_args.mm_use_im_start_end\n",
    "model.config.mm_use_im_patch_token = model_args.mm_use_im_patch_token\n",
    "model.initialize_vision_tokenizer(model_args, tokenizer=tokenizer)\n",
    "log_rank0(\"Vision modules initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.train.train_fsdp import make_supervised_data_module\n",
    "\n",
    "log_rank0(\"Configuring data module...\")\n",
    "assert model.get_model().get_vision_tower().num_patches == data_args.image_token_len, (model.get_model().get_vision_tower().num_patches, data_args.image_token_len)\n",
    "data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataset': <llava.train.train_fsdp.LazySupervisedDataset at 0x7f03e41964a0>,\n",
       " 'eval_dataset': None,\n",
       " 'data_collator': DataCollatorForSupervisedDataset(tokenizer=LlamaTokenizer(name_or_path='lmsys/vicuna-7b-v1.5', vocab_size=32000, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }, image_token_len=196, image_position=35)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llava.train.train_fsdp.LazySupervisedDataset at 0x7f03e41964a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = data_module[\"train_dataset\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'image'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "item.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([136]), torch.Size([136]), torch.Size([3, 224, 224]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item[\"input_ids\"].shape, item[\"labels\"].shape, item[\"image\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(10):\n",
    "    data.append(train_dataset[i])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'labels', 'attention_mask', 'position_ids', 'images'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = data_module[\"data_collator\"]\n",
    "batch = data_collator(data)\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([10, 2048])\n",
      "labels torch.Size([10, 2048])\n",
      "attention_mask torch.Size([10, 2048])\n",
      "position_ids torch.Size([10, 2048])\n",
      "images torch.Size([10, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for k, v in batch.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, labels, attention_mask, position_ids, images = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"], batch[\"position_ids\"], batch[\"images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 224, 224])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_images = torch.cat([image for image in images], dim=0)\n",
    "concat_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 196, 1024])\n",
      "torch.Size([10, 196, 4096])\n"
     ]
    }
   ],
   "source": [
    "self = model\n",
    "image_features1 = self.get_model().get_vision_tower()(images)\n",
    "print(image_features1.shape)\n",
    "image_features = self.get_model().mm_projector(image_features1)\n",
    "print(image_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llava.model.multimodal_encoder.siglip_encoder import SiglipVisionTower\n",
    "\n",
    "# # SiglipVisionTower.__base__.__base__\n",
    "\n",
    "# issubclass(model.get_model().get_vision_tower().__class__, SiglipVisionTower.__base__.__base__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.vision_tower.num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaConfig {\n",
       "  \"_name_or_path\": \"lmsys/vicuna-7b-v1.5\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"freeze_mm_mlp_adapter\": false,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"image_aspect_ratio\": \"pad\",\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 11008,\n",
       "  \"max_position_embeddings\": 4096,\n",
       "  \"mm_hidden_size\": 1152,\n",
       "  \"mm_patch_merge_type\": \"flat\",\n",
       "  \"mm_projector_lr\": null,\n",
       "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
       "  \"mm_use_im_patch_token\": false,\n",
       "  \"mm_use_im_start_end\": false,\n",
       "  \"mm_vision_select_feature\": \"patch\",\n",
       "  \"mm_vision_select_layer\": -2,\n",
       "  \"mm_vision_tower\": \"timm/ViT-SO400M-14-SigLIP\",\n",
       "  \"mm_vision_tower_lr\": null,\n",
       "  \"model_type\": \"llava_llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_model_max_length\": 2048,\n",
       "  \"tokenizer_padding_side\": \"right\",\n",
       "  \"torch_dtype\": \"float16\",\n",
       "  \"transformers_version\": \"4.39.2\",\n",
       "  \"tune_mm_mlp_adapter\": false,\n",
       "  \"unfreeze_mm_vision_tower\": false,\n",
       "  \"use_cache\": false,\n",
       "  \"use_mm_proj\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
