{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import CLIPVisionModel, CLIPImageProcessor, CLIPVisionConfig\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer \n",
    "\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"openai/clip-vit-large-patch14-336\"\n",
    "\n",
    "image_processor = CLIPImageProcessor.from_pretrained(model)\n",
    "vision_tower = CLIPVisionModel.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_layer = -2\n",
    "select_feature = 'patch'\n",
    "unfreeze_mm_vision_tower = False\n",
    "\n",
    "device = vision_tower.device\n",
    "dtype = vision_tower.dtype\n",
    "\n",
    "def feature_select(image_forward_outs):\n",
    "    image_features = image_forward_outs.hidden_states[select_layer]\n",
    "    if select_feature == 'patch':\n",
    "        image_features = image_features[:, 1:]\n",
    "    elif select_feature == 'cls_patch':\n",
    "        image_features = image_features\n",
    "    else:\n",
    "        raise ValueError(f'Unexpected select feature: {select_feature}')\n",
    "    return image_features\n",
    "\n",
    "def _forward(images):\n",
    "    with torch.set_grad_enabled(unfreeze_mm_vision_tower):\n",
    "        image_forward_outs = vision_tower(images.to(device=device, dtype=dtype), output_hidden_states=True)\n",
    "        image_features = feature_select(image_forward_outs).to(images.dtype)\n",
    "        return image_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.5806, -0.5806, -0.5806,  ..., -0.7850, -0.7850, -0.7704],\n",
       "          [ 0.5581,  0.5581,  0.5435,  ..., -0.4054, -0.4054, -0.4200],\n",
       "          [ 0.8355,  0.8501,  0.8355,  ..., -0.2010, -0.2156, -0.2156],\n",
       "          ...,\n",
       "          [ 0.8792,  0.9084,  0.9230,  ...,  1.0106,  0.9960,  0.9814],\n",
       "          [ 0.9084,  0.8938,  0.9522,  ...,  1.0252,  1.0106,  1.0398],\n",
       "          [ 0.7187,  0.6895,  0.7479,  ...,  0.6457,  0.6603,  0.6749]],\n",
       "\n",
       "         [[-0.6715, -0.6715, -0.6715,  ..., -0.8516, -0.8516, -0.8516],\n",
       "          [-0.1613, -0.1313, -0.1463,  ..., -1.0317, -1.0317, -1.0317],\n",
       "          [ 0.0488,  0.0789,  0.0789,  ..., -0.8967, -0.8967, -0.8967],\n",
       "          ...,\n",
       "          [ 0.9343,  0.9643,  0.9793,  ...,  1.0994,  1.0844,  1.0694],\n",
       "          [ 0.9643,  0.9493,  1.0093,  ...,  1.1144,  1.0994,  1.1294],\n",
       "          [ 0.7842,  0.7392,  0.7992,  ...,  0.7242,  0.7392,  0.7542]],\n",
       "\n",
       "         [[-0.4422, -0.4422, -0.4422,  ..., -0.5986, -0.5986, -0.5986],\n",
       "          [ 0.0840,  0.1124,  0.0982,  ..., -0.6412, -0.6412, -0.6555],\n",
       "          [ 0.2973,  0.3115,  0.3115,  ..., -0.4990, -0.4990, -0.4990],\n",
       "          ...,\n",
       "          [ 1.1505,  1.1789,  1.1932,  ...,  1.2927,  1.2785,  1.2643],\n",
       "          [ 1.1789,  1.1647,  1.2216,  ...,  1.3069,  1.2927,  1.3211],\n",
       "          [ 0.9941,  0.9656,  1.0083,  ...,  0.9372,  0.9514,  0.9656]]]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = image_processor(raw_image, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4228, -0.9447,  0.2154,  ..., -0.0067,  1.1019,  0.6007],\n",
       "         [-0.0308,  0.3509,  0.7939,  ...,  0.6656,  0.9412, -0.7258],\n",
       "         [-0.1157,  0.1328,  1.3532,  ...,  0.9491, -0.3748,  0.7546],\n",
       "         ...,\n",
       "         [ 0.5602,  0.2668,  0.6287,  ...,  0.1729,  0.4725, -0.0097],\n",
       "         [ 0.6587, -1.0553, -0.8035,  ..., -0.1626,  0.2400,  1.1361],\n",
       "         [ 0.4978,  0.1702, -0.3934,  ...,  0.4429,  0.3947, -0.2092]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = _forward(inputs.pixel_values)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 576, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = inputs.pixel_values\n",
    "\n",
    "image_forward_outs = vision_tower(images.to(device=device, dtype=dtype), output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.modeling_outputs.BaseModelOutputWithPooling"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image_forward_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_forward_outs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 577, 1024])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_forward_outs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 577, 1024])\n",
      "1 torch.Size([1, 577, 1024])\n",
      "2 torch.Size([1, 577, 1024])\n",
      "3 torch.Size([1, 577, 1024])\n",
      "4 torch.Size([1, 577, 1024])\n",
      "5 torch.Size([1, 577, 1024])\n",
      "6 torch.Size([1, 577, 1024])\n",
      "7 torch.Size([1, 577, 1024])\n",
      "8 torch.Size([1, 577, 1024])\n",
      "9 torch.Size([1, 577, 1024])\n",
      "10 torch.Size([1, 577, 1024])\n",
      "11 torch.Size([1, 577, 1024])\n",
      "12 torch.Size([1, 577, 1024])\n",
      "13 torch.Size([1, 577, 1024])\n",
      "14 torch.Size([1, 577, 1024])\n",
      "15 torch.Size([1, 577, 1024])\n",
      "16 torch.Size([1, 577, 1024])\n",
      "17 torch.Size([1, 577, 1024])\n",
      "18 torch.Size([1, 577, 1024])\n",
      "19 torch.Size([1, 577, 1024])\n",
      "20 torch.Size([1, 577, 1024])\n",
      "21 torch.Size([1, 577, 1024])\n",
      "22 torch.Size([1, 577, 1024])\n",
      "23 torch.Size([1, 577, 1024])\n",
      "24 torch.Size([1, 577, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(image_forward_outs.hidden_states)):\n",
    "    print(i, image_forward_outs.hidden_states[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(image_forward_outs.hidden_states[-2] == image_forward_outs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
