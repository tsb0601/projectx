DeepSpeed (X) -> Impossible
FlashAttention (X) -> TODOs: JAX implementation FlashAttention
Gradient Accumulation (X) -> ?

Stage1
A100*8: ~20hrs
TPU-V4-128: ~20hrs

Stage2
A100*8: ~40-50hrs
TPU-V4-128: TODO

Parallel:
1. Naive (Works, maybe not the most ideal)
2. FSDP (It will increase memory usage)
3. SPMD (TPU-V4-8)
    without SPMD: 12*4 = 48 (300hrs)
    with SPMD: 90 (faster) (200hrs)
    TODO: TPU Pod
    TODO: Explore strategies in SPMD
    TODO: Automatic SPMD

